{% extends "layout.html" %}

{% block content %}
<div class="container">
  <h1>Our Methods</h1>
  <p>
    This page details our data set and methodology for extracting the metadata
    surfaced by this web application.
  </p>
  <hr>
</div>

<div class="container">
  <h3>The Dataset</h3>
  <p>
    Our data comes from the Internet Archive's
    <a target="_blank" href="https://archive.org/details/tv">TV News Archive</a>,
    from 2010 onwards. We use metadata for the three major cable TV news channels
    (CNN, FOX, and MSNBC) in the US, providing near 24/7 video for the last decade.
    Up to 3 minutes of video may be played back. Metadata such as "show",
    "airtime", and closed captions are provided by the Internet Archive. For
    more information about the TV News Archive, please consult the
    <a target="_blank" href="https://archive.org/">Internet Archive</a>
    directly.
  </p>
  <hr>

  <h3>Use of "AI" and Computer Vision</h3>
  <p>
    We use "AI"-based methods to augment the basic metadata provided by the
    Internet Archive. The additional metadata labels that are we computed with
    these methods include bounding boxes on faces; the gender and identity
    of detected faces; whether a person is a host; etc.
    These detectors have imperfect accuracy and can produce noisy labels.
  </p>

  <hr>
  <h5>Faces</h5>
  <p>
    To detect faces in the video, we use the
    <a target="_blank" href="https://github.com/davidsandberg/facenet">FaceNet</a>
    face detector on one frame for every 3 seconds of video. Note that the
    model is a face detector and not a general person detector.
    For example, the model will may fail to detect faces that are turned
    away from the camera, out of focus, or extremely small in the frame.
  </p>

  <p>
    With the faces that are detected, compute FaceNet embedding vectors.
    These embeddings are used as features to train a k-Nearest Neighbors
    classifier for gender, which achieves 97% accuracy on independently
    sampled hold-out data.
  </p>

  <!-- <p>
    When presenting results in the application, we restrict queries to faces
    whose bounding boxes are at least 20% of the frame's height. This is to
    filter our small faces that are in the backgrounds or in the chyrons.
    The face detector's precision and recall on faces taller than 20% of the
    frame's height is 98% and 97%, respecively.
  </p> -->

  <p>
    It is important to recognize that our gender label accuracy statistics
    are averages across a random sample of the data set. Confounding factors
    such as a single person's appearance can skew the accuracy on particular
    slices of the data set. Our tool only supports binary gender labels,
    computed from a face's visual appearance; this may or may not be
    consistent with each person's self-definition of gender.
  </p>

  <h5>Identity Labels</h5>
  <p>
    The majority of the idenitity labels are computed using Amazon's
    Celebrity Recognition API provided under the
    <a target="_blank" href="https://aws.amazon.com/rekognition/">Amazon Rekognition</a>
    service. Amazon has provided this service free of charge for our purposes.
    The labels from Amazon are noisy; individuals who have similar appearances
    will sometimes be confused with each other, leading to false positives,
    and the detector will also sometimes miss instances of an individual.
  </p>

  <h5>Person Tags</h5>
  <p>
    Metadata person-tags are downloaded from
    <a target="_blank" href="https://wiki.dbpedia.org/">DBpedia</a>, using their
    public SPARQL API. We associate metadata from DBpedia with the names of faces
    provided by Amazon Rekognition. This process is automatic and can have errors;
    for instance, if multiple individuals share the same name, then the
    tags may correspond to the wrong person. Likewise, for some
    individuals, no entries are available on DBpedia. This process also
    only applies to faces for which we have a known face identity label.
  </p>

  <hr>
  <h3>Transcripts</h3>
  <p>
    Closed captions are provided by the Internet Archive. We use the
    <a target="_blank" href="https://lowerquality.com/gentle/">Gentle word aligner</a>
    to closely align the words in the captions to the audio track.
  </p>

  <hr>
  <h3>Commercials</h3>
  <p>
    The video data set includes content and commericals. Commercials are
    excluded from the query results. To exclude commercials, we use a
    complex-events query that incorporates information from the visual frames
    and the transcripts. The commercial detector's precision and recall are
    93.0% and 96.8%.
  </p>
</div>
{% endblock %}
