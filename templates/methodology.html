{% extends "layout.html" %}

{% block content %}
<div class="container">
  <h1>Methodology</h1>
  <p>
    This page documents the methodology used for extracting metadata from the cable TV news video dataset.
  </p>
  <hr>
</div>

<div class="container">
  <h3>The Dataset</h3>
  <p>
The cable TV news dataset available for analysis of the Stanford Cable TV News Analyzer (referred to as “the dataset” in the remainder of this document) was obtained from the Internet Archive’s <a href="https://archive.org/details/tv">TV News Archive</a>. The dataset currently includes near 24-7 recordings of CNN, FOX, and MSNBC between January 1, 2010 and July 23, 2019. In total, the dataset consists of over 230,000 hours of video and includes both TV news programming and commercial segments.  
</p>
<p>
The data we obtained from The Internet Archive is organized as a collection of videos, with each video corresponding to one airing of a news program (e.g., most videos are approximately one hour in length). Per-video metadata includes the name of the news program, the date/time it aired, and the channel on which it aired. Video frames range in resolution from 640x360 to 858x480.  All videos include audio. 95.4% of the video files have accompanying closed caption transcripts. However, transcripts are not present during most commercial segments.
</p>
<p>
  Please consult <a href="https://archive.org/">The Internet Archive</a> directly for more information about the process used by the Internet Archive to record the dataset.
</p>

<hr>
<h3>Face Detection</h3>

<p>We detected faces in video frames using the <a href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html">MTCNN</a> [1,2] face detector.  Due to the high cost of performing face detection on all frames in the dataset, we performed face detection only on a subset of frames uniformly sampled every three seconds in a video.
</p>

<p>
This process yielded 306 million face detections across the dataset. Each face location is represented by an axis-aligned bounding box within its containing frame.  
To remind users that face detection is not occurring on all frames, 
the Stanford Cable TV News Analyzer's video player renders face bounding boxes in the frame in which a detection occurred, then fades out the box over the next three seconds.
</p>

<p>
  For each detected face, we compute the following per-face “tags” and descriptors:
</p>

<hr>

<h3>Face Descriptors</h3>

<p>
We compute the 128-element <a href="https://github.com/davidsandberg/facenet">FaceNet</a> descriptor [3,4] from the pixels contained inside a face’s bounding box. 
</p>

<hr>

<h3>Face Gender Tags</h3>

<p>
  We tag all faces with an estimate of their presented binary gender (male/female) using a binary classifier that operates on a face’s FaceNet descriptor as input.  We acknowledge that treating an individual’s gender as a binary quantity as well as assessing gender solely from an individual’s appearance is a grossly simplified treatment of a complex topic.  However, we believe that this simple model still provides useful insights into the presentation of cable TV news, and for this reason we chose to include it in the dataset.
  </p>

<p>
  Members of our research team manually annotated the presented binary gender of 12,669 faces selected at random from the dataset.  Each face was annotated by a single human annotator. (No additional consensus protocol was used.)  We used these ground-truth annotations to train a binary classifier based on the precomputed FaceNet descriptors. Our implementation is a k-NN classifier with classification results determined by majority vote with k=7.
</p>

<p>
<b>Validation:</b> The classifier's agreement with human annotators on a test set of 1,066 faces 
selected at random from the dataset is given by the confusion matrix below. In this test set, 735 of these faces were assessed by human annotator to be male presenting. 331 were assessed by humans to be female.
</p>

<p style="text-align: center"><img src="{{ url_for('static', filename='img/validation/gender_confusion.png') }}" height="300"/></p>

<hr>

<h3>Face Identity Tags</h3>

<p>
  We use <a href="https://docs.aws.amazon.com/rekognition/latest/dg/celebrities.html">Amazon Rekognition's Celebrity Recognition API</a> to attempt to identify all detected faces. The Celebrity Recognition API returns an identity prediction for 45.2% of the faces in the dataset.  The Celebrity Recognition API provides an identity estimate only when its identification confidence score is greater than 0.5. We use all predictions above this 0.5 threshold, and do no additional thresholding.  Identities in the dataset result from Celebrity Recognition API queries performed in September 2019.
</p>

<p>
To increase the percentage of faces with identity tags, we tag faces that were not identified by the Celebrity Recognition API but have close visual similarity to identified faces. This is accomplished with a nearest neighbor classifier on a per video basis; we take the faces that have no label from the Celebrity Recognition API, and find all of their neighbors (if they exist)  in face embedding space with L2 distance under a conservative threshold, and then take the majority vote of those labels as the label. In total 55.5% of the faces in the dataset contain an identity tag. 5,500 identities receive at least 10 minutes of screen time. The Stanford Cable TV News Analyzer limits queries to use only these identities.
</p>

<hr>

<h3>Additional Face Tags</h3>

<p>
  For each face tagged with an identity, we download all person-tags associated with this identity from <a href="https://wiki.dbpedia.org/">DBpedia</a> [5] using its public <a href="https://wiki.dbpedia.org/public-sparql-endpoint">SPARQL API</a>.  A full description of the per-face tags available in the Stanford Cable TV News Analyzer is available on the <a href="/data/tags">tag information page</a>.  Tags listed with source of "DBpedia" on that page were directly downloaded from DBpedia.  
</p>

<p>
We also augment tags downloaded from DBpedia with additional "derived" tags computed from DBpedia tags.  For example, the derived tag "african_american" is formed from the union of DBpedia tags featuring the prefix "african_american": e.g., "african_american_academics", "african_american_mayors".  
</p>

<hr>

<h3>Commercial Segment Detection</h3>

<p>
We detect commercial segments using a proprietary commercial detection algorithm that uses heuristics based on visual information (the presence of black frames at the start/end of commercials) and transcript text features (word casing).  We authored the algorithm in <a href="https://github.com/scanner-research/rekall">Rekall</a>, an API for complex event detection in video. 
</p>

<p>
  Overall, there are 70,559 hours of detected commercials in the dataset, leaving 182,896 hours of program content.  By default, video segments lying within commercials are excluded from query results in the Stanford Cable TV News Analyzier. 
</p>

<p>
<b>Validation:</b> We hand annotated all commercial segments in a 20-hour dataset. (6.3 hours of this time fell within commercial segments.)  On this test set, the commercial detector achieves a precision of 93.0% and a recall of 96.8%.  Precision and recall were computed on a per-frame basis (not in terms of commercial instances). Specifically, the precision of commercial detection is computed as the faction of frames in the test set that the classifier correctly classified as commercials divided by the total number of frames the classifier estimated were part of a commercial segment.) 
</p>

<hr>

<h3>Transcript Time Alignment</h3>

<p>
We use the <a href="https://lowerquality.com/gentle/">Gentle word aligner</a> to perform sub-second alignment of words in a video's closed-caption transcript to the video's audio track.  (The source transcripts are only coarsely aligned to the video.)  To perform alignment, we partition the video's audio track into one-second chunks and use Gentle to search for word-level alignment with transcript text within +/- 10 seconds of this audio segment.
</p>

<hr>

<h3>Screen Time Estimates</h3>

<p>
The result of a query performed using the Stanford Cable TV News Analyzer is an estimate of screen time of video segments matching the query.  Below we describe how we estimate screen time for face-based and transcript-text-based predicates.
</p>

<p>Since face detection is performed on frames spaced by three seconds in a video, the Stanford Cable TV News Analyzer tabulates face screen time at a granularity of three seconds.  For example, a face detection of Anderson Cooper in a single video frame contributes three seconds to the estimate of Mr. Cooper's screen time in the video.   
</p>

<p>
The Stanford Cable TV News Analyzer tabulates the screen time of transcript words (as opposed to counts of the number of occurrences of these words) to support queries that select video segments using a composition of face-based and transcript-based predicates.  The site tabulates the screen time of transcript-text queriesusing the duration of words determined via transcript time alignment. For example, an utterance of the word "politics" that begins at 10:35.10 in a video and ends at 10:35.90 contributes 0.8 seconds to the estimate of the word's total screen time in the video.  Since the Stanford Cable TV News Analyzer tabulates the screen time of transcript words matching a query, the screen time estimate for a longer word (e.g., "Mississippi") will likely be greater than that of a shorter word (e.g., "Iraq") even if the two words are spoken the same number of times.
</p>

<hr>

<h3>References</h3>

<p>
  [1] MTCNN Source Page (<a href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html">https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html</a>)
</p>

<p>
[2] Zhang, Kaipeng, et al. "Joint face detection and alignment using multitask cascaded convolutional networks." IEEE Signal Processing Letters 23.10 (2016): 1499-1503.
</p>

<p>
  [3] FaceNet TensorFlow Implementation (<a href="https://github.com/davidsandberg/facenet">https://github.com/davidsandberg/facenet</a>)
</p>

<p>
  [4] F. Schroff, D. Kalenichenko and J. Philbin, "FaceNet: A unified embedding for face recognition and clustering," 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, 2015, pp. 815-823.
doi: 10.1109/CVPR.2015.7298682
URL: <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298682&isnumber=7298593">http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298682&isnumber=7298593</a>
</p>

<p>
[5] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. DBpedia: a nucleus for a web of open data. In Proceedings of the 6th international The semantic web and 2nd Asian conference on Asian semantic web conference (ISWC'07/ASWC'07).
</p>

</div>
{% endblock %}
